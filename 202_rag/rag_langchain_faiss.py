#!/usr/bin/env python3
"""
Kepler 442c Sci-Fi RAG Example - LangChain + FAISS
Korean sci-fi world knowledge base with RAG
"""

import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
import torch
import os
import math

# Memory optimization
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

class LangChainKoreanRAG:
    def __init__(self, model_path: str = "openai/gpt-oss-20b", csv_path: str = "kepler_442c_data.csv",
                 similarity_threshold: float = 0.3, max_docs: int = 3, distance_scale: float = 100.0):
        """Initialize LangChain-based RAG system

        Args:
            similarity_threshold: Minimum probability score to consider a document relevant (0-1, higher = more similar)
            max_docs: Maximum number of documents to retrieve
            distance_scale: Scale factor for exponential decay conversion (lower = more sensitive)
        """

        print("ğŸš€ Kepler 442c Sci-Fi RAG with LangChain + FAISS")
        print("=" * 50)

        # Store parameters
        self.similarity_threshold = similarity_threshold
        self.max_docs = max_docs
        self.distance_scale = distance_scale

        print(f"ğŸ“Š Probability threshold: {similarity_threshold}")
        print(f"ğŸ“„ Max documents: {max_docs}")
        print(f"ğŸ”§ Distance scale: {distance_scale}")

        # Clear GPU cache
        torch.cuda.empty_cache()

        # Load documents
        print("ğŸ“„ Loading documents...")
        self.documents = self.load_documents(csv_path)

        # Initialize embeddings
        print("ğŸ”¢ Setting up embeddings...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name='jhgan/ko-sroberta-multitask',
            model_kwargs={'device': 'cpu'}
        )

        # Create FAISS vector store
        print("ğŸ—ƒï¸ Creating FAISS vector store...")
        self.vectorstore = self.create_faiss_store()

        # Initialize LLM
        print("ğŸ¤– Setting up LLM...")
        self.llm = self.setup_llm(model_path)

        # Create QA chain
        print("â›“ï¸ Building QA chain...")
        self.qa_chain = self.create_qa_chain()

        print("âœ… Ready!")

    def load_documents(self, csv_path: str) -> list[Document]:
        """Load CSV data and convert to LangChain Documents"""
        df = pd.read_csv(csv_path)

        documents = []
        for _, row in df.iterrows():
            content = f"ì œëª©: {row['title']}\n\në‚´ìš©: {row['content']}"

            doc = Document(
                page_content=content,
                metadata={
                    "id": str(row['id']),
                    "title": row['title'],
                    "source": "korean_fermentation_data"
                }
            )
            documents.append(doc)

        print(f"Loaded {len(documents)} documents")
        return documents

    def create_faiss_store(self) -> FAISS:
        """Create FAISS vector store with documents"""

        # Text splitter for chunking
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=50,
            separators=["\n\n", "\n", ". ", " "]
        )

        split_docs = text_splitter.split_documents(self.documents)
        print(f"Split into {len(split_docs)} chunks")

        # Create FAISS vector store
        vectorstore = FAISS.from_documents(
            documents=split_docs,
            embedding=self.embeddings
        )

        print(f"FAISS index created with {vectorstore.index.ntotal} vectors")
        return vectorstore

    def setup_llm(self, model_path: str) -> HuggingFacePipeline:
        """Setup local LLM with LangChain wrapper"""

        try:
            # 8-bit quantization
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
            )

            # Load tokenizer and model
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                cache_dir="/home/anderson/.cache/huggingface/hub"
            )

            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=quantization_config,
                device_map="auto",
                cache_dir="/home/anderson/.cache/huggingface/hub",
                low_cpu_mem_usage=True,
                torch_dtype=torch.float16
            )

            # Create pipeline
            text_pipeline = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=150,
                temperature=0.7,
                do_sample=True,
                return_full_text=False,
                pad_token_id=tokenizer.eos_token_id
            )

            # LangChain wrapper
            llm = HuggingFacePipeline(pipeline=text_pipeline)
            print("LLM loaded with 8-bit quantization")
            return llm

        except Exception as e:
            print(f"LLM loading failed: {e}")
            print("Using retrieval-only mode")
            return None

    def create_qa_chain(self) -> RetrievalQA:
        """Create LangChain RetrievalQA chain"""

        # Korean prompt template
        template = """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

        # Create FAISS retriever
        retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )

        if self.llm is None:
            return retriever

        # Create QA chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )

        return qa_chain

    def distance_to_probability(self, distance: float) -> float:
        """Convert FAISS distance to probability using exponential decay

        Args:
            distance: FAISS distance score (lower = more similar)

        Returns:
            probability: Score from 0 to 1 (higher = more similar)
        """
        return math.exp(-distance / self.distance_scale)

    def query(self, question: str) -> dict:
        """Query the RAG system with probability-based filtering"""

        # Get more documents than needed, then filter by threshold
        all_docs_with_scores = self.vectorstore.similarity_search_with_score(question, k=10)

        # Convert distances to probabilities
        docs_with_probs = []
        for doc, distance in all_docs_with_scores:
            probability = self.distance_to_probability(distance)
            docs_with_probs.append((doc, distance, probability))

        # Filter by probability threshold
        filtered_docs = [(doc, distance, prob) for doc, distance, prob in docs_with_probs
                        if prob >= self.similarity_threshold]

        # Limit to max_docs
        docs_with_scores = filtered_docs[:self.max_docs]

        print(f"ğŸ“Š Found {len(all_docs_with_scores)} total docs, {len(filtered_docs)} above probability threshold ({self.similarity_threshold:.2f}), using top {len(docs_with_scores)}")

        if self.llm is None:
            # Retrieval only
            if docs_with_scores:
                best_doc, best_distance, best_prob = docs_with_scores[0]
                title = best_doc.metadata.get('title', 'ì œëª© ì—†ìŒ')
                content = best_doc.page_content.split('ë‚´ìš©: ')[1] if 'ë‚´ìš©: ' in best_doc.page_content else best_doc.page_content
                answer = f"[{title}] {content}"
            else:
                answer = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í™•ë¥  ì„ê³„ê°’ë³´ë‹¤ ë‚®ì€ ë¬¸ì„œë“¤ë§Œ ìˆìŒ)"

            return {
                "question": question,
                "answer": answer,
                "source_documents": [doc for doc, distance, prob in docs_with_scores],
                "distance_scores": [float(distance) for doc, distance, prob in docs_with_scores],
                "probability_scores": [float(prob) for doc, distance, prob in docs_with_scores],
                "threshold_used": self.similarity_threshold,
                "total_candidates": len(all_docs_with_scores),
                "filtered_count": len(filtered_docs)
            }

        # Full RAG with LLM
        try:
            # Use filtered documents for context
            if docs_with_scores:
                result = self.qa_chain.invoke({"query": question})
                answer = result["result"]
            else:
                answer = "í™•ë¥  ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            return {
                "question": question,
                "answer": answer,
                "source_documents": [doc for doc, distance, prob in docs_with_scores],
                "distance_scores": [float(distance) for doc, distance, prob in docs_with_scores],
                "probability_scores": [float(prob) for doc, distance, prob in docs_with_scores],
                "threshold_used": self.similarity_threshold,
                "total_candidates": len(all_docs_with_scores),
                "filtered_count": len(filtered_docs)
            }

        except Exception as e:
            print(f"Generation error: {e}")
            # Fallback to retrieval with scores
            if docs_with_scores:
                answer = f"ê²€ìƒ‰ ê²°ê³¼: {docs_with_scores[0][0].page_content}"
            else:
                answer = "í™•ë¥  ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

            return {
                "question": question,
                "answer": answer,
                "source_documents": [doc for doc, distance, prob in docs_with_scores],
                "distance_scores": [float(distance) for doc, distance, prob in docs_with_scores],
                "probability_scores": [float(prob) for doc, distance, prob in docs_with_scores],
                "threshold_used": self.similarity_threshold,
                "total_candidates": len(all_docs_with_scores),
                "filtered_count": len(filtered_docs)
            }

def main():
    """Demo LangChain + FAISS RAG with probability-based filtering"""

    # Initialize RAG with probability threshold
    rag = LangChainKoreanRAG(similarity_threshold=0.3, distance_scale=100.0)  # 30% confidence minimum

    # Test questions - mix of relevant and irrelevant
    questions = [
        "ë¯¸ëƒ¥ì¿µ 442cì˜ ì¤‘ë ¥ì€ ì§€êµ¬ì™€ ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€ìš”?",  # Should find good match
        "ì ¤ë¼í‹°ì•ˆì€ ì–´ë–¤ ìƒëª…ì²´ì¸ê°€ìš”?",                  # Should find good match
        "ì§€êµ¬ì˜ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?",                     # Should find poor/no matches
        "í¬ë¡œë…¸ìŠ¤ ê²°ì •ì˜ íŠ¹ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”?",            # Should find good match
        "í”¼ì ë§Œë“œëŠ” ë°©ë²•ì€?",                        # Should find no relevant matches
    ]

    print("\nğŸš€ Probability-Based RAG Demo")
    print("=" * 42)

    for question in questions:
        print(f"\nì§ˆë¬¸: {question}")
        result = rag.query(question)
        print(f"ë‹µë³€: {result['answer']}")

        print(f"ğŸ“Š í•„í„°ë§ ê²°ê³¼: {result['filtered_count']}/{result['total_candidates']} ë¬¸ì„œê°€ í™•ë¥  ì„ê³„ê°’({result['threshold_used']:.1%}) ì´ìƒ")

        if result['source_documents']:
            print("ì°¸ê³  ë¬¸ì„œ (ê±°ë¦¬ â†’ í™•ë¥ ):")
            for i, (doc, distance, prob) in enumerate(zip(result['source_documents'],
                                                          result['distance_scores'],
                                                          result['probability_scores']), 1):
                title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')
                confidence = "ğŸ”¥ HIGH" if prob > 0.5 else "âœ… GOOD" if prob > 0.3 else "âš ï¸ OK" if prob > 0.1 else "âŒ LOW"
                print(f"  {i}. {title}")
                print(f"     ê±°ë¦¬: {distance:.1f} â†’ í™•ë¥ : {prob:.1%} {confidence}")
        else:
            print("âŒ í™•ë¥  ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        print()

def test_different_thresholds():
    """Test with different similarity thresholds"""
    print("\nğŸ”¬ Testing Different Thresholds")
    print("=" * 35)

    question = "ì§€êµ¬ì˜ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?"  # Irrelevant question
    thresholds = [200.0, 150.0, 120.0, 100.0, 80.0]

    for threshold in thresholds:
        print(f"\nì„ê³„ê°’: {threshold}")
        rag = LangChainKoreanRAG(similarity_threshold=threshold)
        result = rag.query(question)
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(result['source_documents'])}")
        if result['source_documents']:
            best_score = result['similarity_scores'][0]
            print(f"ìµœê³  ì ìˆ˜: {best_score:.2f}")

if __name__ == "__main__":
    main()
    # Uncomment to test different thresholds
    # test_different_thresholds()