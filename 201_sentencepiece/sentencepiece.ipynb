{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c3588b-a614-4a5d-8c66-2fc72d8142cf",
   "metadata": {},
   "source": [
    "# Sentece Piece \n",
    "\n",
    "- unsupervised text tokenizer and detokenizer 이며 주로 딥러닝에서 사용\n",
    "- 내부적으로 BPE (byte-pair-encoding) 그리고 unigram language model 을 사용\n",
    "- 특정 언어에 국한되지 않고, 다양한 언어에 사용 가능\n",
    "\n",
    "- [논문](https://arxiv.org/pdf/1808.06226.pdf)\n",
    "- [Github](https://github.com/google/sentencepiece)\n",
    "\n",
    "설치\n",
    "\n",
    "```\n",
    "$ sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev\n",
    "$ pip install sentencepiece\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4795df53-5efb-4394-b54b-8bf57eec68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import gettempdir\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sentencepiece as stp\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fdd962-7e55-4cfd-8729-d553c2791418",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa2dc31-6d0f-4982-87ec-065db3561c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: /tmp/nsmc_train.txt\n",
      "test_path : /tmp/nsmc_test.txt\n",
      "train_df  : (149995, 4)\n",
      "test_df   : (49997, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>morph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29288</th>\n",
       "      <td>7796626</td>\n",
       "      <td>\"아이언맨3 보고난후 제목이 연관되어서 보게됐는데 \"\"아이언\"\" 들어가는 영화는 대...</td>\n",
       "      <td>1</td>\n",
       "      <td>아이언맨 3 보고 난후 제목 이 연관 되어서 보게 됐는데 아이언 들어가는 영화 는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>8366483</td>\n",
       "      <td>쇼타군 넘 좋아 돈키호테 대박 재밌음</td>\n",
       "      <td>1</td>\n",
       "      <td>쇼타 군 넘 좋아 돈키호테 대박 재밌음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39568</th>\n",
       "      <td>7554622</td>\n",
       "      <td>그럭저럭 맹숭하다.</td>\n",
       "      <td>0</td>\n",
       "      <td>그럭저럭 맹숭하 다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34664</th>\n",
       "      <td>8098518</td>\n",
       "      <td>1편의 빈 디젤에 비해서 2편의 아이스 큐브는 너무나 약해보이고, 액션도 전혀 인상...</td>\n",
       "      <td>0</td>\n",
       "      <td>1 편의 빈 디젤 에 비 해서 2 편의 아이스 큐브 는 너무나 약 해보이고 액션 도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8048</th>\n",
       "      <td>8349698</td>\n",
       "      <td>♥예수님사랑해요오오오오</td>\n",
       "      <td>1</td>\n",
       "      <td>예수님 사랑 해 요 오오오오</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           document  label  \\\n",
       "29288  7796626  \"아이언맨3 보고난후 제목이 연관되어서 보게됐는데 \"\"아이언\"\" 들어가는 영화는 대...      1   \n",
       "76     8366483                               쇼타군 넘 좋아 돈키호테 대박 재밌음      1   \n",
       "39568  7554622                                         그럭저럭 맹숭하다.      0   \n",
       "34664  8098518  1편의 빈 디젤에 비해서 2편의 아이스 큐브는 너무나 약해보이고, 액션도 전혀 인상...      0   \n",
       "8048   8349698                                       ♥예수님사랑해요오오오오      1   \n",
       "\n",
       "                                                   morph  \n",
       "29288  아이언맨 3 보고 난후 제목 이 연관 되어서 보게 됐는데 아이언 들어가는 영화 는 ...  \n",
       "76                                 쇼타 군 넘 좋아 돈키호테 대박 재밌음  \n",
       "39568                                         그럭저럭 맹숭하 다  \n",
       "34664  1 편의 빈 디젤 에 비 해서 2 편의 아이스 큐브 는 너무나 약 해보이고 액션 도...  \n",
       "8048                                     예수님 사랑 해 요 오오오오  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download(url, filename, force=False):\n",
    "    path = Path(gettempdir()) / filename\n",
    "    if not path.exists() or force:\n",
    "        with open(path, \"wt\") as f:\n",
    "            r = requests.get(url, allow_redirects=True)\n",
    "            f.write(r.text)\n",
    "\n",
    "    df = pd.read_csv(path, delimiter=\"\\t\")\n",
    "    return path, df\n",
    "\n",
    "\n",
    "def preprocess_morph(text) -> str:\n",
    "    morphs = okt.pos(str(text))\n",
    "    tokens = []\n",
    "    for word, pos in morphs:\n",
    "        if pos in (\"Punctuation\", \"Foreign\"):\n",
    "            continue\n",
    "\n",
    "        tokens.append(word)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n",
    "test_url = \"https://github.com/e9t/nsmc/raw/master/ratings_test.txt\"\n",
    "\n",
    "train_path, train_df = download(train_url, \"nsmc_train.txt\")\n",
    "test_path, test_df = download(test_url, \"nsmc_test.txt\")\n",
    "\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "train_df[\"morph\"] = train_df.document.apply(preprocess_morph)\n",
    "test_df[\"morph\"] = test_df.document.apply(preprocess_morph)\n",
    "\n",
    "print(\"train_path:\", train_path)\n",
    "print(\"test_path :\", test_path)\n",
    "print(f\"train_df  : {train_df.shape}\")\n",
    "print(f\"test_df   : {test_df.shape}\")\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd591c-9878-436c-b4da-7c83f074fe99",
   "metadata": {},
   "source": [
    "# Sentencepiece with text file\n",
    "\n",
    "## Train with text File \n",
    "- input: 학습 파일 위치\n",
    "- model_prefix: 모델이름\n",
    "- vocab_size: vocabulary 단어 크기\n",
    "- model_type: `unigram` (default) | `bpe` | `char` | `word`\n",
    "- max_sentence_length: 문장 최대 길이\n",
    "- pad_id: pad token ID\n",
    "- unk_id: unknown token ID\n",
    "- bos_id: Begin of sentence token ID\n",
    "- eos_id: End of sentence token ID \n",
    "- user_defined_symbols: 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23675fee-0e96-4150-824b-a325ccda00d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /tmp/sentencepiece-train.txt\n",
      "  input_format: \n",
      "  model_prefix: /tmp/nsmc-sentencepiece\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: foo\n",
      "  user_defined_symbols: bar\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /tmp/sentencepiece-train.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 149995 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: foo\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: bar\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=5864407\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=1580\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 149995 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 85847 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 149995\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 101061\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 101061 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=54431 obj=9.62624 num_tokens=185628 num_tokens/piece=3.41034\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=43765 obj=8.92716 num_tokens=185818 num_tokens/piece=4.24581\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=32818 obj=8.89126 num_tokens=195203 num_tokens/piece=5.94805\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=32798 obj=8.87207 num_tokens=195200 num_tokens/piece=5.95158\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=24598 obj=8.93683 num_tokens=208090 num_tokens/piece=8.45963\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=24597 obj=8.92211 num_tokens=208087 num_tokens/piece=8.45985\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=18447 obj=9.01752 num_tokens=221026 num_tokens/piece=11.9817\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=18447 obj=8.99776 num_tokens=221034 num_tokens/piece=11.9821\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13835 obj=9.13329 num_tokens=234337 num_tokens/piece=16.938\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13835 obj=9.10637 num_tokens=234337 num_tokens/piece=16.938\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=10376 obj=9.2774 num_tokens=248585 num_tokens/piece=23.9577\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=10376 obj=9.24293 num_tokens=248585 num_tokens/piece=23.9577\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=7782 obj=9.45509 num_tokens=262932 num_tokens/piece=33.7872\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=7782 obj=9.40986 num_tokens=262971 num_tokens/piece=33.7922\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5836 obj=9.68423 num_tokens=277899 num_tokens/piece=47.6181\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5836 obj=9.6272 num_tokens=277899 num_tokens/piece=47.6181\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=4400 obj=9.96454 num_tokens=294638 num_tokens/piece=66.9632\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=4400 obj=9.88996 num_tokens=294640 num_tokens/piece=66.9636\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: /tmp/nsmc-sentencepiece.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: /tmp/nsmc-sentencepiece.vocab\n"
     ]
    }
   ],
   "source": [
    "train_morph_path = Path(gettempdir()) / \"sentencepiece-train.txt\"\n",
    "model_prefix_path = Path(gettempdir()) / \"nsmc-sentencepiece\"\n",
    "train_df.morph.to_csv(train_morph_path, index=False, header=False)\n",
    "\n",
    "stp.SentencePieceTrainer.train(\n",
    "    input=train_morph_path,\n",
    "    model_prefix=model_prefix_path,\n",
    "    vocab_size=4000,\n",
    "    user_defined_symbols=[\"foo\", \"bar\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29820ced-4c68-4c60-9e5f-0eb74972c825",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c82d63b4-3cb9-4c78-8404-f8cfe0d90b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text             : 극장 포스터 에 손수건 을 갖고오세요 라고 써있던게 기억나네요\n",
      "Encode          : [638, 885, 10, 765, 89, 206, 11, 1126, 15, 238, 1515, 258, 963, 409, 131, 30, 261, 46, 111]\n",
      "Encode as IDs   : [638, 885, 10, 765, 89, 206, 11, 1126, 15, 238, 1515, 258, 963, 409, 131, 30, 261, 46, 111]\n",
      "Encode as Pieces: ['▁극장', '▁포스터', '▁에', '▁손', '수', '건', '▁을', '▁갖', '고', '오', '세요', '▁라고', '▁써', '있', '던', '게', '▁기억', '나', '네요']\n",
      "Decode from IDs : 극장 포스터 에 손수건 을 갖고오세요 라고 써있던게 기억나네요\n"
     ]
    }
   ],
   "source": [
    "sp = stp.SentencePieceProcessor()\n",
    "sp.load(str(model_prefix_path.with_suffix(\".model\")))\n",
    "\n",
    "\n",
    "text = test_df.sample().morph.values[0]\n",
    "print('Text             :', text)\n",
    "print('Encode          :', sp.Encode(text))\n",
    "print('Encode as IDs   :', sp.EncodeAsIds(text))\n",
    "print('Encode as Pieces:', sp.EncodeAsPieces(text))\n",
    "print('Decode from IDs :', sp.decode(sp.Encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74755b1a-dd27-4369-ab0a-c64ffe4ed854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['드라마 의 감동 을 깨지 않으려면 영화 는 보지마라',\n",
       " '보느니 후레쉬맨 보는게 낫다 그래픽 특수분장 완죤 티 난다',\n",
       " '썩 괜찮은 영화 인디 영화 의 매력 이란 게 이런거 아니겠어 잊을수 없는 조중동 디스 ㅋㅋㅋㅋㅋㅋㅋ']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Encode]\n",
      "[[73, 8, 78, 11, 1067, 18, 551, 2181, 109, 7, 14, 2810], [157, 1707, 115, 331, 458, 1425, 869, 1580, 1613, 1676, 2903, 237, 230, 1774, 3288, 1013, 1404], [2370, 915, 7, 38, 377, 7, 8, 352, 701, 72, 2109, 272, 625, 45, 5, 2284, 149, 89, 108, 187, 369, 388, 709, 61, 2685]]\n",
      "\n",
      "[Encode as Pieces]\n",
      "[['▁드라마', '▁의', '▁감동', '▁을', '▁깨', '지', '▁않', '으려', '면', '▁영화', '▁는', '▁보지마라'], ['▁보', '느', '니', '▁후', '레', '쉬', '맨', '▁보는게', '▁낫다', '▁그래픽', '▁특수', '분', '장', '▁완', '죤', '▁티', '▁난다'], ['▁썩', '▁괜찮은', '▁영화', '▁인', '디', '▁영화', '▁의', '▁매력', '▁이란', '▁게', '▁이런거', '▁아니', '겠', '어', '▁', '잊', '을', '수', '▁없는', '▁조', '중', '동', '▁디', '스', '▁ᄏᄏᄏᄏᄏᄏᄏ']]\n",
      "\n",
      "[Decode]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['드라마 의 감동 을 깨지 않으려면 영화 는 보지마라',\n",
       " '보느니 후레쉬맨 보는게 낫다 그래픽 특수분장 완죤 티 난다',\n",
       " '썩 괜찮은 영화 인디 영화 의 매력 이란 게 이런거 아니겠어 잊을수 없는 조중동 디스 ᄏᄏᄏᄏᄏᄏᄏ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = test_df.sample(3).morph.tolist()\n",
    "\n",
    "print('[Text]')\n",
    "display(text_list)\n",
    "\n",
    "\n",
    "print('\\n[Encode]')\n",
    "encoded = sp.encode(text_list)\n",
    "print(encoded)\n",
    "\n",
    "print('\\n[Encode as Pieces]')\n",
    "print([sp.encode_as_pieces(line) for line in text_list])\n",
    "\n",
    "print('\\n[Decode]')\n",
    "sp.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e171ce0-16ac-4d3f-a760-e3f920b72e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
