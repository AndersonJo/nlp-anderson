{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70560ef7-2730-4719-9e65-f3b195d5298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f973c2-0dee-43b3-9539-550b281fa0fa",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2eb0ec-bfa4-404b-8d5e-babe91d74888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Tokenizer Path: /tmp/kobert/kobert-news-wiki.spiece\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tempfile import gettempdir\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "\n",
    "def _download(bucket: str, key: str, target):\n",
    "    cache_path = Path(gettempdir()) / \"kobert\" / target\n",
    "\n",
    "    # Download the file\n",
    "    if not cache_path.parent.exists():\n",
    "        os.makedirs(cache_path.parent)\n",
    "\n",
    "    if cache_path.exists():\n",
    "        return str(cache_path)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=None,\n",
    "        aws_secret_access_key=None,\n",
    "        config=Config(signature_version=UNSIGNED),\n",
    "    )\n",
    "\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        s3.download_fileobj(bucket, key, f)\n",
    "    return str(cache_path)\n",
    "\n",
    "\n",
    "def download_tokenizer():\n",
    "    bucket = \"skt-lsl-nlp-model\"\n",
    "    key = \"KoBERT/tokenizers/kobert_news_wiki_ko_cased-1087f8699e.spiece\"\n",
    "    return _download(bucket, key, \"kobert-news-wiki.spiece\")\n",
    "\n",
    "\n",
    "tokenizer_path = download_tokenizer()\n",
    "print(\"Downloaded Tokenizer Path:\", tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d3533-9298-468f-9449-4d15678f0a89",
   "metadata": {},
   "source": [
    "## Load a Sentencepiece Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f2dc4f-0065-4628-9b6f-79006e4966f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode         : [4617, 7576, 7086, 1967, 7143]\n",
      "EncodeAsIds    : [4617, 7576, 7086, 1967, 7143]\n",
      "EncodeAsPieces : ['▁치', '킨', '은', '▁맛', '있다']\n",
      "Decode         : 치킨\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as stp\n",
    "\n",
    "tokenizer = stp.SentencePieceProcessor(model_file=tokenizer_path)\n",
    "\n",
    "print(\"Encode         :\", tokenizer.Encode(\"치킨은 맛있다\"))\n",
    "print(\"EncodeAsIds    :\", tokenizer.EncodeAsIds(\"치킨은 맛있다\"))\n",
    "print(\"EncodeAsPieces :\", tokenizer.EncodeAsPieces(\"치킨은 맛있다\"))\n",
    "print(\"Decode         :\", tokenizer.Decode(tokenizer.Encode(\"치킨\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf9617-6be3-41a5-bead-04d880f84437",
   "metadata": {},
   "source": [
    "## Load a Sentencepiece Model by GluonNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff7dba6c-47ad-4b51-b231-6a69f8dc0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp as gnlp\n",
    "\n",
    "vocab = gnlp.vocab.BERTVocab.from_sentencepiece(\n",
    "    tokenizer_path, padding_token=\"[PAD]\"\n",
    ")\n",
    "tokenizer = gnlp.data.BERTSPTokenizer(tokenizer_path, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e8d48b6-30f8-4a96-a1c7-fd21089cea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncodeAsPieces: ['▁치', '킨', '은', '▁맛', '있다']\n",
      "EncodeAsIds   : [4617, 7576, 7086, 1967, 7143]\n",
      "Decode        : ['▁치', '킨', '은', '▁맛', '있다']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"치킨은 맛있다\")\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "decodes = vocab.to_tokens(ids)\n",
    "\n",
    "print(\"EncodeAsPieces:\", tokens)\n",
    "print(\"EncodeAsIds   :\", ids)\n",
    "print(\"Decode        :\", decodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54eab6-a7b3-4afa-bdea-fd733bb01c51",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "https://nlp.gluon.ai/api/modules/data.html\n",
    "\n",
    "## BERTSentenceTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4cce299c-f462-43db-a49f-27e06583315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[token_ids]\n",
      " [   2 4928 5778 7096 2812 3748 2590 7782 6493 1725 6542 7158 4213 6604\n",
      " 7076 3658 1185 6116  517 6266 5760 3886 6142  517 6202 6165 7819 3149\n",
      " 3376 6542 7088  517 6869 5400  517 7806 4924 6745 7101    3    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1]\n",
      "\n",
      "[valid_length]\n",
      " 40\n",
      "\n",
      "[segment_ids]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "[id -> token]\n",
      " ['[CLS]', '▁하나', '님', '이', '▁세상을', '▁이처럼', '▁사랑', '하', '사', '▁독', '생', '자를', '▁주', '셨', '으니', '▁이는', '▁그', '를', '▁', '믿', '는', '▁자', '마다', '▁', '멸', '망', '하지', '▁않고', '▁영', '생', '을', '▁', '얻', '게', '▁', '하려', '▁하', '심', '이라', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "transform = gnlp.data.BERTSentenceTransform(\n",
    "    tokenizer,  # gluonnlp.data.transforms.BERTSPTokenizer\n",
    "    max_seq_length=64,  # 문장의 길이\n",
    "    pad=True,\n",
    "    pair=False,\n",
    ")\n",
    "\n",
    "text = \"하나님이 세상을 이처럼 사랑하사 독생자를 주셨으니 이는 그를 믿는 자마다 멸망하지 않고 영생을 얻게 하려 하심이라\"\n",
    "token_ids, valid_length, segment_ids = transform([text])\n",
    "print('[token_ids]\\n', token_ids)\n",
    "print('\\n[valid_length]\\n', valid_length)\n",
    "print('\\n[segment_ids]\\n', segment_ids)\n",
    "print('\\n[id -> token]\\n', tokenizer.vocab.to_tokens(token_ids.tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d00c97-b241-41b9-99c6-afe8981748cc",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3041ce68-404e-477d-b31e-be3224bcaa6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertModel"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "def download_kobert(ctx: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    :param cts: \"cpu\" or \"cuda:0\"\n",
    "    \"\"\"\n",
    "    device = torch.device(ctx)\n",
    "    bucket = \"skt-lsl-nlp-model\"\n",
    "    key = \"KoBERT/models/kobert_v1.zip\"\n",
    "    zip_path = _download(bucket, key, \"kobert_v1.zip\")\n",
    "    zip_path = Path(zip_path)\n",
    "    zipf = ZipFile(zip_path)\n",
    "    zipf.extractall(path=zip_path.parent)\n",
    "\n",
    "    model_path = zip_path.parent / \"kobert_from_pretrained\"\n",
    "    bertmodel = BertModel.from_pretrained(model_path, return_dict=False)\n",
    "    bertmodel.to(device)\n",
    "    bertmodel.eval()\n",
    "    return bertmodel\n",
    "\n",
    "\n",
    "bert = download_kobert()\n",
    "type(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2e92ccd8-5ae0-4bb0-aded-cf3645aca14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text          : ['▁손흥민', '은', '▁대한민국', '▁국', '적', '의', '▁토', '트', '넘', '▁', '홋', '스', '퍼', '▁', 'FC', '▁소속', '▁축구', '선수']\n",
      "expected token: [2866, 7086, 1683, 1132, 7202, 7095, 4737, 7659, 5698, 517, 0, 6664, 7706, 517, 286, 2837, 4562, 6562]\n",
      "actual   token: [[2, 2866, 7086, 1683, 1132, 7202, 7095, 4737, 7659, 5698, 517, 0, 6664, 7706, 517, 286, 2837, 4562, 6562, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "valid_length  : 20\n",
      "\n",
      "Text          : ['▁최', '상', '위', '▁제품', '인', '▁', '라이', '젠', '▁9', '▁', '79', '50', 'X', '3', 'D', '는', '▁16', '코', '어', '▁', ',', '▁32', '스', '레드', '로', '▁작동', '하며', '▁최대', '▁작동', '▁클', '록', '은', '▁5', '▁', '.', '▁7', 'G', 'H', 'z', '다']\n",
      "expected token: [4519, 6527, 7044, 4158, 7119, 517, 6011, 7241, 627, 517, 218, 176, 359, 142, 278, 5760, 545, 7533, 6855, 517, 46, 597, 6664, 6051, 6079, 3934, 7810, 4527, 3934, 4689, 6083, 7086, 611, 517, 54, 621, 290, 294, 459, 5782]\n",
      "actual   token: [[2, 4519, 6527, 7044, 4158, 7119, 517, 6011, 7241, 627, 517, 218, 176, 359, 142, 278, 5760, 545, 7533, 6855, 517, 46, 597, 6664, 6051, 6079, 3934, 7810, 4527, 3934, 4689, 6083, 7086, 611, 517, 54, 621, 290, 294, 459, 5782, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "valid_length  : 42\n",
      "\n",
      "similarity: -0.028257260099053383\n",
      "MSE : 293.84893798828125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as F\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def predict(text):\n",
    "    print('Text          :', tokenizer(text[0]))\n",
    "    print('expected token:', tokenizer.convert_tokens_to_ids(tokenizer(text[0])))\n",
    "    token_ids, valid_length, segment_ids = transform(text)\n",
    "    token_ids = torch.Tensor([token_ids]).long().to(device)\n",
    "    segment_ids = torch.Tensor([segment_ids]).long().to(device)\n",
    "    valid_length = valid_length\n",
    "    print('actual   token:', token_ids.tolist())\n",
    "    print('valid_length  :', valid_length)\n",
    "    print()\n",
    "\n",
    "    attention_mask = torch.zeros_like(token_ids)\n",
    "    attention_mask[0][:valid_length] = 1\n",
    "    attention_mask = attention_mask.float().to(device)\n",
    "\n",
    "    _, out = bert(\n",
    "        input_ids=token_ids,\n",
    "        token_type_ids=segment_ids,\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "cos_f = F.CosineSimilarity()\n",
    "\n",
    "a = predict([\"손흥민은 대한민국 국적의 토트넘 홋스퍼 FC 소속 축구선수\"])\n",
    "b = predict([\"최상위 제품인 라이젠 9 7950X3D는 16코어, 32스레드로 작동하며 최대 작동 클록은 5.7GHz다\"])\n",
    "\n",
    "print(\"similarity:\", cos_f(a, b).item())\n",
    "print(\"MSE :\", ((b - a) ** 2).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "16cd14ac-c4d6-4d82-acb8-cb11616e0338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text          : ['▁소', '떡', '▁맛', '나', '▁치', '킨']\n",
      "expected token: [2822, 5970, 1967, 5655, 4617, 7576]\n",
      "actual   token: [[2, 2822, 5970, 1967, 5655, 4617, 7576, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "valid_length  : 8\n"
     ]
    }
   ],
   "source": [
    "a = predict([\"소떡 맛나 치킨\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b291922b-4a5e-4c52-8964-548ad4b5c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text          : ['▁간', '▁장', '▁치', '▁', '킨', '▁피', '▁자']\n",
      "expected token: [777, 3954, 4617, 517, 7576, 4909, 3886]\n",
      "actual   token: [[2, 777, 3954, 4617, 517, 7576, 4909, 3886, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "valid_length  : 9\n"
     ]
    }
   ],
   "source": [
    "a = predict([\"간 장 치 킨 피 자\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyEnv 3.12.9",
   "language": "python",
   "name": "3.12.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
