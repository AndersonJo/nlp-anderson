# MXFP4, Hopper, BF16 그리고 왜 에러가 났는가

아래는 GPT-OSS + Unsloth 환경에서 MXFP4 커널이 터지는 사례를 기준으로 정리한 기술 메모입니다.  
문장은 짧게, 하지만 핵심이 빠지지 않게 구성했습니다.

---

## 작성 플랜

1. **문제 정의**: 어떤 에러가 어떤 조건에서 발생했는지 요약  
2. **핵심 개념**: MXFP4·Hopper·BF16을 중심으로 필요한 배경만 설명  
3. **관련 용어 리스트**: 자주 등장하는 정밀도·아키텍처·커널 키워드 정리  
4. **원인 분석**: 왜 Hopper 전용 경로에서 실패했는지 구조적으로 설명  
5. **해결 전략**: 실전에서 가장 빠른 우회와 검증 포인트 정리  

---

## 문제 요약

에러 메시지의 요지는 “**Hopper 전용 스위즐만 지원**”이라는 것입니다.  
즉, 현재 커널이 **Hopper 전용 경로**로 들어갔는데, 실제 실행 환경이 그 조건을 만족하지 못했습니다.  
이때 가장 안전한 우회는 **MXFP4 경로를 피하고 BF16 체크포인트로 전환**하는 것입니다.

---

## 핵심 개념 (짧고 핵심만)

### MXFP4
- **혼합 정밀도 4-bit 포맷**입니다.  
- 모델 가중치를 극도로 압축해 **메모리·대역폭**을 줄입니다.  
- 대신 **특수 커널**과 **스케일 관리**가 필요합니다.  

### Hopper
- NVIDIA **H100/Hopper 세대**를 의미합니다.  
- **TMA(비동기 메모리 이동)**, **FP8 텐서코어** 등 특화 기능이 있습니다.  
- 특정 **메모리 스위즐**과 커널 경로가 Hopper에만 맞습니다.  

### BF16
- **FP16보다 지수 범위가 넓은 16-bit** 형식입니다.  
- 수치 안정성이 좋아 **추론·학습에서 안전한 기본값**으로 자주 씁니다.  
- 특별한 하드웨어 경로 없이도 폭넓게 동작합니다.  

---

## 관련 키워드 리스트 (간결 설명)

### 정밀도·포맷
- **FP32**: 표준 부동소수점. 느리지만 안정적.  
- **TF32**: FP32의 속도 타협형. NVIDIA 전용 최적화.  
- **FP16**: 속도·메모리 절약형. 범위는 좁음.  
- **BF16**: FP16보다 안전한 범위. 대체 기본값.  
- **FP8 (E4M3/E5M2)**: Hopper 이후 핵심 저정밀 포맷.  
- **MXFP4**: 더 극단적 4-bit 포맷. 커널 의존도 높음.  
- **INT8/INT4**: 정수 양자화. 빠르지만 정밀도 손실 큼.  
- **NF4**: 4-bit 양자화 변형. QLoRA에서 자주 사용.  

### 아키텍처
- **Ampere (A100)**: FP16/BF16 중심의 성숙한 플랫폼.  
- **Ada (L40 등)**: 추론 지향. FP8 일부 지원.  
- **Hopper (H100)**: TMA·FP8·스위즐 최적화 핵심 세대.  
- **Blackwell (B100/GB200 등)**: FP8/MX 계열 강화, 도입기.  

### 커널·가속
- **Triton**: 커스텀 GPU 커널 DSL. 성능 민감.  
- **GEMM**: 행렬 곱. 대부분의 모델 연산 핵심.  
- **Swizzle**: 메모리 레이아웃 최적화 기법. 하드웨어 의존.  
- **TMA**: Hopper 전용 고속 메모리 이동 유닛.  

### 모델 구조·런타임
- **MoE**: 여러 전문가 중 일부만 활성화.  
- **Router/Experts**: MoE 내부 선택 및 분기.  
- **KV Cache**: 디코딩 속도 핵심 캐시.  
- **LoRA/QLoRA**: 경량 미세조정 기법.  

---

## 왜 Hopper 전용 스위즐 에러가 뜨나

1. **MXFP4 경로는 전용 Triton 커널**로 가는 경우가 많습니다.  
2. 그 커널은 **Hopper 전용 스위즐**을 가정합니다.  
3. 실제 환경이 Hopper 조건을 만족하지 못하면 **컴파일 타임에서 바로 실패**합니다.  
4. 그래서 오류가 모델 로딩 직후, 혹은 첫 번째 forward에서 터집니다.  

핵심은 “**커널 경로 선택**”입니다.  

---

## 커널 경로란 무엇인가 (조금 더 자세히)

커널 경로는 **같은 연산을 어떤 구현으로 실행할지 결정되는 실행 분기**입니다.  
겉으로는 `matmul` 한 줄이지만, 내부에서는 여러 후보 커널이 경쟁합니다.  

### 커널 경로를 바꾸는 대표 신호들
- **정밀도**: BF16/FP16/FP8/MXFP4에 따라 경로가 달라집니다.  
- **하드웨어 세대**: Ampere/Hopper/Blackwell마다 가능한 커널이 다릅니다.  
- **라이브러리 버전**: Triton·Transformers·Unsloth 조합이 분기 조건이 됩니다.  
- **옵션/플래그**: `load_in_4bit`, `dtype`, `use_cache` 같은 값도 경로에 영향합니다.  

### 왜 이게 중요하나
- 커널은 **컴파일 타임에 조건을 강하게 가정**합니다.  
- 가정이 틀리면 **실행 전/초기에 바로 실패**합니다.  
- 그래서 “동일 코드가 GPU만 바꿔도 망가지는” 현상이 생깁니다.  

### 한 줄로 정리
커널 경로는 **성능을 위해 만든 지름길**이고,  
하드웨어가 맞지 않으면 **막힌 지름길**이 됩니다.  

---

## 실전 해결 전략

### 1) 가장 빠른 우회
- **MXFP4 체크포인트 대신 BF16 체크포인트 사용**  
- 커널 경로가 단순해지고 호환성이 급격히 좋아집니다.  

### 2) 재현·검증 포인트
- 모델 로딩 직후 **첫 forward**에서 터지면 커널 문제일 확률이 큽니다.  
- 로그에 **swizzle / Triton / matmul / mxfp**가 보이면 거의 확정입니다.  

### 3) 장기 대응
- Hopper 전용 커널이 아니라면 **fallback 경로**가 필요합니다.  
- 특정 라이브러리 버전 조합이 스위즐 옵션을 바꿀 수 있습니다.  
- 가장 안전한 방법은 **정밀도 경로를 단순화**하는 것입니다.  

---

## 예제 코드 (대표적이고 실전적인 선택)

**추천 선택: PyTorch + Transformers + Unsloth**  
Triton은 로우레벨 커널 최적화 용도라 블로그 예제로는 과합니다.  
실무에서 가장 많이 쓰는 상위 스택이 이 조합입니다.  

```python
from unsloth import FastLanguageModel
from transformers import TextStreamer

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/gpt-oss-20b-BF16",
    dtype="bfloat16",
    max_seq_length=2000,
    load_in_4bit=False,
    full_finetuning=False,
    low_cpu_mem_usage=True,
    device_map="cuda",
)

messages = [{"role": "user", "content": "Solve x^5 + 3x^4 - 10 = 3."}]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
    return_dict=True,
    reasoning_effort="medium",
).to("cuda")

_ = model.generate(
    **inputs,
    max_new_tokens=512,
    streamer=TextStreamer(tokenizer),
    use_cache=True,
)
```

핵심은 **MXFP4 체크포인트 대신 BF16 체크포인트를 쓰는 것**입니다.  
이 한 줄 선택이 커널 경로를 바꿔 에러를 피하게 합니다.  

---

## 한 줄 요약

MXFP4는 빠르지만 **하드웨어 의존성이 높고**,  
BF16은 느리지만 **예측 가능하고 안정적**입니다.  

현장에서는 **“작동하는 경로를 우선 확보”**한 뒤  
필요할 때만 저정밀 최적화를 다시 켜는 전략이 가장 현실적입니다.  
