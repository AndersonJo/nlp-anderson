{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c3588b-a614-4a5d-8c66-2fc72d8142cf",
   "metadata": {},
   "source": [
    "# Sentece Piece \n",
    "\n",
    "- unsupervised text tokenizer and detokenizer 이며 주로 딥러닝에서 사용\n",
    "- 내부적으로 BPE (byte-pair-encoding) 그리고 unigram language model 을 사용\n",
    "- 특정 언어에 국한되지 않고, 다양한 언어에 사용 가능\n",
    "\n",
    "- [논문](https://arxiv.org/pdf/1808.06226.pdf)\n",
    "- [Github](https://github.com/google/sentencepiece)\n",
    "\n",
    "설치\n",
    "\n",
    "```\n",
    "$ sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev\n",
    "$ pip install sentencepiece\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4795df53-5efb-4394-b54b-8bf57eec68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import gettempdir\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sentencepiece as stp\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fdd962-7e55-4cfd-8729-d553c2791418",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0aa2dc31-6d0f-4982-87ec-065db3561c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: /tmp/nsmc_train.txt\n",
      "test_path : /tmp/nsmc_test.txt\n",
      "train_df  : (49997, 4)\n",
      "test_df   : (49997, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>morph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38829</th>\n",
       "      <td>9068645</td>\n",
       "      <td>포도향이 진하게 전해지는 영화..근20년 지난 지금도..</td>\n",
       "      <td>1</td>\n",
       "      <td>포도 향 이 진하게 전해지는 영화 근 20년 지난 지금 도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48451</th>\n",
       "      <td>9635876</td>\n",
       "      <td>조니뎁 진짜 약빨고 연기한 거 같음</td>\n",
       "      <td>1</td>\n",
       "      <td>조니뎁 진짜 약 빨 고 연기 한 거 같음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43125</th>\n",
       "      <td>9320232</td>\n",
       "      <td>그냥 디즈니 닭 이라고 하자!</td>\n",
       "      <td>0</td>\n",
       "      <td>그냥 디즈니 닭 이라고 하자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22037</th>\n",
       "      <td>3748046</td>\n",
       "      <td>굿.</td>\n",
       "      <td>1</td>\n",
       "      <td>굿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16127</th>\n",
       "      <td>5566461</td>\n",
       "      <td>재미있습니다. 볼만해요.</td>\n",
       "      <td>1</td>\n",
       "      <td>재미있습니다 볼 만해 요</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                         document  label  \\\n",
       "38829  9068645  포도향이 진하게 전해지는 영화..근20년 지난 지금도..      1   \n",
       "48451  9635876              조니뎁 진짜 약빨고 연기한 거 같음      1   \n",
       "43125  9320232                 그냥 디즈니 닭 이라고 하자!      0   \n",
       "22037  3748046                               굿.      1   \n",
       "16127  5566461                    재미있습니다. 볼만해요.      1   \n",
       "\n",
       "                                  morph  \n",
       "38829  포도 향 이 진하게 전해지는 영화 근 20년 지난 지금 도  \n",
       "48451            조니뎁 진짜 약 빨 고 연기 한 거 같음  \n",
       "43125                   그냥 디즈니 닭 이라고 하자  \n",
       "22037                                 굿  \n",
       "16127                     재미있습니다 볼 만해 요  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download(url, filename):\n",
    "    path = Path(gettempdir()) / filename\n",
    "    if not path.exists():\n",
    "        with open(path, \"wt\") as f:\n",
    "            r = requests.get(url, allow_redirects=True)\n",
    "            f.write(r.text)\n",
    "\n",
    "    df = pd.read_csv(test_path, delimiter=\"\\t\")\n",
    "    return path, df\n",
    "\n",
    "\n",
    "def preprocess_morph(text) -> str:\n",
    "    morphs = okt.pos(str(text))\n",
    "    tokens = []\n",
    "    for word, pos in morphs:\n",
    "        if pos in (\"Punctuation\", \"Foreign\"):\n",
    "            continue\n",
    "\n",
    "        tokens.append(word)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n",
    "test_url = \"https://github.com/e9t/nsmc/raw/master/ratings_test.txt\"\n",
    "\n",
    "train_path, train_df = download(train_url, \"nsmc_train.txt\")\n",
    "test_path, test_df = download(test_url, \"nsmc_test.txt\")\n",
    "\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "train_df[\"morph\"] = train_df.document.apply(preprocess_morph)\n",
    "test_df[\"morph\"] = test_df.document.apply(preprocess_morph)\n",
    "\n",
    "print(\"train_path:\", train_path)\n",
    "print(\"test_path :\", test_path)\n",
    "print(f\"train_df  : {train_df.shape}\")\n",
    "print(f\"test_df   : {test_df.shape}\")\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd591c-9878-436c-b4da-7c83f074fe99",
   "metadata": {},
   "source": [
    "# Sentencepiece with text file\n",
    "\n",
    "## Train with text File \n",
    "- input: 학습 파일 위치\n",
    "- model_prefix: 모델이름\n",
    "- vocab_size: vocabulary 단어 크기\n",
    "- model_type: `unigram` (default) | `bpe` | `char` | `word`\n",
    "- max_sentence_length: 문장 최대 길이\n",
    "- pad_id: pad token ID\n",
    "- unk_id: unknown token ID\n",
    "- bos_id: Begin of sentence token ID\n",
    "- eos_id: End of sentence token ID \n",
    "- user_defined_symbols: 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "23675fee-0e96-4150-824b-a325ccda00d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /tmp/sentencepiece-train.txt\n",
      "  input_format: \n",
      "  model_prefix: /tmp/nsmc-sentencepiece\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: foo\n",
      "  user_defined_symbols: bar\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /tmp/sentencepiece-train.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 50000 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: foo\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: bar\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=1963018\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1550\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 50000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 44779 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 50000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 55086\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 55086 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=28770 obj=9.83881 num_tokens=102708 num_tokens/piece=3.56997\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=23738 obj=9.18618 num_tokens=103088 num_tokens/piece=4.34274\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17799 obj=9.19775 num_tokens=109099 num_tokens/piece=6.1295\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17793 obj=9.17326 num_tokens=109105 num_tokens/piece=6.13191\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=13344 obj=9.29426 num_tokens=116998 num_tokens/piece=8.76784\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=13344 obj=9.27072 num_tokens=117002 num_tokens/piece=8.76814\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10008 obj=9.42397 num_tokens=124838 num_tokens/piece=12.4738\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10008 obj=9.39359 num_tokens=124838 num_tokens/piece=12.4738\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=7506 obj=9.57959 num_tokens=132248 num_tokens/piece=17.619\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=7506 obj=9.54083 num_tokens=132248 num_tokens/piece=17.619\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=5629 obj=9.80054 num_tokens=140207 num_tokens/piece=24.908\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=5629 obj=9.75066 num_tokens=140207 num_tokens/piece=24.908\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=4400 obj=10.0385 num_tokens=147444 num_tokens/piece=33.51\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=4400 obj=9.9814 num_tokens=147445 num_tokens/piece=33.5102\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: /tmp/nsmc-sentencepiece.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: /tmp/nsmc-sentencepiece.vocab\n"
     ]
    }
   ],
   "source": [
    "train_morph_path = Path(gettempdir()) / \"sentencepiece-train.txt\"\n",
    "model_prefix_path = Path(gettempdir()) / \"nsmc-sentencepiece\"\n",
    "train_df.morph.to_csv(train_morph_path, index=False, header=False)\n",
    "\n",
    "stp.SentencePieceTrainer.train(\n",
    "    input=train_morph_path,\n",
    "    model_prefix=model_prefix_path,\n",
    "    vocab_size=4000,\n",
    "    user_defined_symbols=[\"foo\", \"bar\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29820ced-4c68-4c60-9e5f-0eb74972c825",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c82d63b4-3cb9-4c78-8404-f8cfe0d90b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text             : 잼있고 신나는 영화 보드 타는 모습 너무 멋져\n",
      "Encode as IDs   : [668, 15, 3089, 19, 7, 118, 145, 501, 19, 421, 23, 1139, 394]\n",
      "Encode as Pieces: ['▁잼있', '고', '▁신나', '는', '▁영화', '▁보', '드', '▁타', '는', '▁모습', '▁너무', '▁멋', '져']\n",
      "Decode from IDs : 잼있고 신나는 영화 보드 타는 모습 너무 멋져\n"
     ]
    }
   ],
   "source": [
    "sp = stp.SentencePieceProcessor()\n",
    "sp.load(str(model_prefix_path.with_suffix(\".model\")))\n",
    "\n",
    "\n",
    "text = test_df.sample().morph.values[0]\n",
    "print('Text             :', text)\n",
    "print('Encode as IDs   :', sp.EncodeAsIds(text))\n",
    "print('Encode as Pieces:', sp.EncodeAsPieces(text))\n",
    "print('Decode from IDs :', sp.decode(sp.Encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "74755b1a-dd27-4369-ab0a-c64ffe4ed854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['주인공 들 너 무답 답 미 리터 놓고말 좀하지 특히 여 주인공',\n",
       " '진짜 재밌게 봤고 다시 봐두 재미 에 감동 임창정 짱 ㅠㅂㅠ',\n",
       " '그때 나 지금 이나 군대 는']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Encode]\n",
      "[[195, 13, 680, 116, 1457, 983, 187, 480, 287, 5, 3993, 15, 413, 111, 889, 607, 146, 195], [34, 236, 5, 3999, 15, 143, 497, 806, 102, 10, 80, 171, 980, 160, 279, 328, 3196], [1309, 30, 205, 323, 688, 158, 14]]\n",
      "\n",
      "[Encode as Pieces]\n",
      "[['▁주인공', '▁들', '▁너', '▁무', '답', '▁답', '▁미', '▁리', '터', '▁', '놓', '고', '말', '▁좀', '하지', '▁특히', '▁여', '▁주인공'], ['▁진짜', '▁재밌게', '▁', '봤', '고', '▁다시', '▁봐', '두', '▁재미', '▁에', '▁감동', '▁임', '창', '정', '▁짱', '▁ᅲ', '뷰'], ['▁그때', '▁나', '▁지금', '▁이나', '▁군', '대', '▁는']]\n",
      "\n",
      "[Decode]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['주인공 들 너 무답 답 미 리터 놓고말 좀하지 특히 여 주인공',\n",
       " '진짜 재밌게 봤고 다시 봐두 재미 에 감동 임창정 짱 ᅲ뷰',\n",
       " '그때 나 지금 이나 군대 는']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = test_df.sample(3).morph.tolist()\n",
    "\n",
    "print('[Text]')\n",
    "display(text_list)\n",
    "\n",
    "\n",
    "print('\\n[Encode]')\n",
    "encoded = sp.encode(text_list)\n",
    "print(encoded)\n",
    "\n",
    "print('\\n[Encode as Pieces]')\n",
    "print([sp.encode_as_pieces(line) for line in text_list])\n",
    "\n",
    "print('\\n[Decode]')\n",
    "sp.decode(encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
